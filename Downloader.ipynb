{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from gensim.utils import deaccent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloaderBezRealitky():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        For the bezrealitky search, you need to iterate over search pages. Self.page_bezrealitky stores the maximum amount\n",
    "        of pages and then via self.link a list of all pages from search is created in self.hrefs_bezrealitky.\n",
    "        '''\n",
    "        self.link = 'https://www.bezrealitky.cz/vypis/nabidka-pronajem/byt/praha?_token=pr1lf-vKwDFfmFbICiz2PfC-Zdwq-2JolXi4MeMHsrw&page=1'\n",
    "        self.request = requests.get(self.link)\n",
    "        self.request.encoding='UTF-8'\n",
    "        self.soup = BeautifulSoup(self.request.text,'lxml')\n",
    "        \n",
    "        self.page_bezrealitky = int(self.soup.findAll('a',{'class':'page-link pagination__page'})[-2].text)\n",
    "        self.hrefs_bezrealitky = ['https://www.bezrealitky.cz/vypis/nabidka-pronajem/byt/praha?_token=pr1lf-vKwDFfmFbICiz2PfC-Zdwq-2JolXi4MeMHsrw&page=' \n",
    "                                  + str(i) for i in range(1,self.page_bezrealitky)]\n",
    "        self.soups = []\n",
    "        \n",
    "    def get_soups(self):\n",
    "        '''\n",
    "        This method iterates over all search pages, converts them into a BeautifulSoup object and stores them in a txt file as \n",
    "        strings outside of this script. BREAKHERE is used to distinguish between objects. \n",
    "        '''\n",
    "        for link in self.hrefs_bezrealitky:\n",
    "            sleep(1)\n",
    "            self.link = link\n",
    "            self.request = requests.get(self.link)\n",
    "            self.request.encoding='UTF-8'\n",
    "            self.soups.append(BeautifulSoup(self.request.text,'lxml'))\n",
    "        with open('bezrealitky_links.txt', 'w') as f:\n",
    "            for s in self.soups:\n",
    "                f.write(str(deaccent(s).encode(\"utf-8\")) + 'BREAKHERE')\n",
    "            f.close\n",
    "        \n",
    "    def get_data(self):\n",
    "        '''\n",
    "        Main method to obtain and transform the data. HTMLs are read from the txt file and stored in a list (soup_list) \n",
    "        within this script. Next, the method iterates over the list, converts the strings in txt file into a BeautifulSoup\n",
    "        object and parses the html for relevant data. At the end, a nested dictionary (dicts) is created and stored\n",
    "        as a json file outside of this script.\n",
    "        '''\n",
    "        with open('bezrealitky_links.txt', 'r') as f:\n",
    "            content = f.read()\n",
    "        soup_list = content.split('BREAKHERE')\n",
    "        dicts = {}\n",
    "        counter = 0\n",
    "        for soup in soup_list:\n",
    "            descrips = [] #empty list for apartment values\n",
    "            values = [] #empty list for apartment prices\n",
    "            vals = BeautifulSoup(soup,'lxml').findAll('strong', {'class':'product__value'}) #parsing for apartment values\n",
    "            ##vals = soup.findAll('strong', {'class':'product__value'})\n",
    "            for vl in vals:\n",
    "                values.append(vl.text.strip())\n",
    "            #img = soup.findAll('img')\n",
    "            img = BeautifulSoup(soup,'lxml').findAll('img') #parsing for apartment info (street, city, size..)\n",
    "            for i in img:\n",
    "                if 'Pronajem' and 'obr. c. 1' in i['alt']: #info present at all pictures, let's take info from the first one\n",
    "                        info = i['alt'].split(',')[0:4] #info separated by comma, split into a list\n",
    "                        if 'Praha' == info[-1].strip(): #if street non present, insert a NaN instead\n",
    "                            info.insert(2, 'NaN')\n",
    "                            del info[-1]\n",
    "                            m = info[1].split(' ')\n",
    "                            info[1] = m[1]\n",
    "                            descrips.append(info)\n",
    "                        else:\n",
    "                            m = info[1].split(' ')\n",
    "                            info[1] = m[1]\n",
    "                            descrips.append(info)\n",
    "            count = 0\n",
    "            for pp in values: #append apartment prices to info about apartments in list descrips\n",
    "                try:\n",
    "                    descrips[count].append(pp)\n",
    "                    descrips[count][0] = descrips[count][0][-4:].strip()\n",
    "                    count += 1\n",
    "                except IndexError:\n",
    "                    count += 1\n",
    "                    continue\n",
    "            for item in descrips:\n",
    "                try:\n",
    "                    if '+' in item[4]: #prices often written as '19000 Kč + 4000Kč' so we need to split it\n",
    "                        prices = item.pop(4).split('+')\n",
    "                        item.append(re.sub(\"[^0-9]\", \"\", prices[0])) #keep only numeric characters, i.e. price\n",
    "                        item.append(re.sub(\"[^0-9]\", \"\", prices[1]))\n",
    "                    else:\n",
    "                        prices = [item.pop(4), '0'] #if only '19000 Kč', insert 0 as price for utilities not specified\n",
    "                        item.append(re.sub(\"[^0-9]\", \"\", prices[0]))\n",
    "                        item.append(re.sub(\"[^0-9]\", \"\", prices[1]))\n",
    "                except IndexError:\n",
    "                    continue\n",
    "            for item in descrips: #store apartment info, price into a dictionary and index by counter\n",
    "                try:\n",
    "                    dict = {}\n",
    "                    dict['Size'] = item[0]\n",
    "                    dict['m2'] = re.sub(\"[^0-9]\", \"\", item[1]) #keep only size, i.e. numeric characters\n",
    "                    dict['Street'] = deaccent(item[2]) #deaccent to provent potential errors\n",
    "                    dict['District'] = deaccent(item[3])\n",
    "                    dict['Base Price'] = int(item[4])\n",
    "                    dict['Utilities Price'] = int(item[5])\n",
    "                    dict['Total Price'] = int(item[4]) + int(item[5])\n",
    "                    dicts[counter] = dict\n",
    "                    counter += 1\n",
    "                except IndexError:\n",
    "                    counter +=1\n",
    "                    continue\n",
    "            print('Done loop number ' + str(counter) + '. Printing descrips.')\n",
    "            print(descrips)\n",
    "        with open('bezrealitky.json', 'w') as write_file: #store data into a json file\n",
    "            json.dump(dicts, write_file, indent = 4)\n",
    "\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DownloaderBezRealitky()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.get_soups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloaderReality():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        For the reality search, you need to iterate over search pages. Self.page_reality stores the maximum amount\n",
    "        of pages and then via self.link a list of all pages from search is created in self.hrefs_reality.\n",
    "        '''\n",
    "        self.link = 'https://reality.idnes.cz/s/pronajem/byty/praha/?page=1'\n",
    "        self.request = requests.get(self.link)\n",
    "        self.request.encoding='UTF-8'\n",
    "        self.soup = BeautifulSoup(self.request.text,'lxml')\n",
    "        \n",
    "        self.page_reality = int(self.soup.findAll('a',{'class':'btn btn--border paging__item'})[-1].text) - 1\n",
    "        self.hrefs_reality = ['https://reality.idnes.cz/s/pronajem/byty/praha/?page=' \n",
    "                                  + str(i) for i in range(1,self.page_reality)]\n",
    "        self.soups = []\n",
    "        \n",
    "    def get_soups(self):\n",
    "        '''\n",
    "        This method iterates over all search pages, converts them into a BeautifulSoup object and stores them in a txt file as \n",
    "        strings outside of this script. BREAKHERE is used to distinguish between objects. \n",
    "        '''\n",
    "        for link in self.hrefs_reality[0:2]:\n",
    "            sleep(1)\n",
    "            self.link = link\n",
    "            self.request = requests.get(self.link)\n",
    "            self.request.encoding='UTF-8'\n",
    "            self.soups.append(BeautifulSoup(self.request.text,'lxml'))\n",
    "            print('Page saved.')\n",
    "        with open('reality_idnes_links.txt', 'w') as f:\n",
    "            for s in self.soups:\n",
    "                f.write(str(deaccent(s).encode(\"utf-8\")) + 'BREAKHERE')\n",
    "            f.close\n",
    "    def get_data(self):\n",
    "        '''\n",
    "        Main method to obtain and transform the data. HTMLs are read from the txt file and stored in a list (soup_list) \n",
    "        within this script. Next, the method iterates over the list, converts the strings in txt file into a BeautifulSoup\n",
    "        object and parses the html for relevant data. At the end, a nested dictionary (dicts) is created and stored\n",
    "        as a json file outside of this script.\n",
    "        '''\n",
    "        with open('reality_idnes_links.txt', 'r') as f:\n",
    "            content = f.read()\n",
    "        soup_list = content.split('BREAKHERE')\n",
    "        dicts = {}\n",
    "        counter = 0\n",
    "        for soup in soup_list[0:2]:\n",
    "            descrips = [] #empty list for apartment values\n",
    "            values = [] #empty list for apartment prices\n",
    "            vals = BeautifulSoup(soup,'lxml').findAll('p', {'class':'c-list-products__price'}) #parsing for apartment values\n",
    "            for vl in vals:\n",
    "                values.append(re.sub(\"[^0-9]\", \"\",vl.find('strong').text))\n",
    "            info = BeautifulSoup(soup,'lxml').findAll('p', {'class':'c-list-products__info'})\n",
    "            for i in info:\n",
    "                if 'Komercni sdeleni' in i.text:\n",
    "                    continue\n",
    "                else:\n",
    "                    temp_info = str(i.text)\n",
    "                    temp_info = re.sub(r'^(?:\\\\n)+','', temp_info).strip()[:-2]\n",
    "                    temp_info = temp_info.strip().split(',')\n",
    "                    temp_info = [i.strip() for i in temp_info]\n",
    "                    print(temp_info)\n",
    "                \n",
    "        \n",
    "            '''\n",
    "            for item in descrips: #store apartment info, price into a dictionary and index by counter\n",
    "                try:\n",
    "                    dict = {}\n",
    "                    dict['Size'] = item[0]\n",
    "                    dict['m2'] = re.sub(\"[^0-9]\", \"\", item[1]) #keep only size, i.e. numeric characters\n",
    "                    dict['Street'] = deaccent(item[2]) #deaccent to provent potential errors\n",
    "                    dict['District'] = deaccent(item[3])\n",
    "                    dict['Base Price'] = int(item[4])\n",
    "                    dict['Utilities Price'] = int(item[5])\n",
    "                    dict['Total Price'] = int(item[4]) + int(item[5])\n",
    "                    dicts[counter] = dict\n",
    "                    counter += 1\n",
    "                except IndexError:\n",
    "                    counter +=1\n",
    "                    continue\n",
    "            print('Done loop number ' + str(counter) + '. Printing descrips.')\n",
    "            print(descrips)\n",
    "        with open('bezrealitky.json', 'w') as write_file: #store data into a json file\n",
    "            json.dump(dicts, write_file, indent = 4)\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = DownloaderReality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page saved.\n",
      "Page saved.\n"
     ]
    }
   ],
   "source": [
    "b.get_soups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['U skladu', 'Praha 4 - Komorany']\n",
      "['Musilkova', 'Praha 5 - Kosire']\n",
      "['Milevska', 'Praha 4 - Krc']\n",
      "['Duskova', 'Praha 5 - Smichov']\n",
      "['Taborska', 'Praha 4 - Nusle', 'okres Praha']\n",
      "['Truhlarska', 'Praha 1 - Nove Mesto']\n",
      "['Spalena', 'Praha 1 - Nove Mesto', 'okres Praha']\n",
      "['Tesnov', 'Praha 1 - Nove Mesto', 'okres Praha']\n",
      "['Nuselska', 'Praha 4 - Nusle', 'okres Praha']\n",
      "['Jankovcova', 'Praha 7 - Holesovice']\n",
      "['Opatovicka', 'Praha 1 - Nove Mesto', 'okres Praha']\n",
      "['Vinohradska', 'Praha 3 - Vinohrady', 'okres Praha']\n",
      "['Balbinova', 'Praha 2 - Vinohrady', 'okres Praha']\n",
      "['Na Kozacce', 'Praha 2 - Vinohrady']\n",
      "['Zverinova', 'Praha 3 - Strasnice', 'okres Praha']\n",
      "['Reznicka', 'Praha 1 - Nove Mesto', 'okres Praha']\n",
      "['Krynicka', 'Praha 8 - Troja', 'okres Praha']\n",
      "['Kalisnicka', 'Praha 3 - Zizkov', 'okres Praha']\n",
      "['Hornomecholupska', 'Praha 10 - Horni Mecholupy', 'okres Praha']\n",
      "['Mrkvickova', 'Praha 6 - Repy']\n",
      "['Korunni', 'Praha 10 - Vinohrady']\n",
      "['Chalabalova', 'Praha 5 - Stodulky']\n",
      "['Smetanovo nabrezi', 'Praha 1 - Stare Mesto']\n",
      "['Ostrovni', 'Praha 1 - Nove Mesto', 'okres Praha']\n",
      "['Ve struhach', 'Praha 6 - Bubenec']\n",
      "['Gutova', 'Praha 10 - Strasnice']\n",
      "['Pisecna', 'Praha 8 - Troja']\n",
      "['Misovicka', 'Praha 5 - Zlicin']\n",
      "['Olsanska', 'Praha 3 - Zizkov']\n",
      "['Hnevkovska', 'Praha 4 - Chodov']\n",
      "['Gabinova', 'Praha 5 - Hlubocepy']\n",
      "['Na hlinach', 'Praha 8 - Kobylisy']\n",
      "['Novakovych', 'Praha 8 - Liben']\n",
      "['Londynska', 'Praha 2 - Vinohrady']\n",
      "['Italska', 'Praha 2 - Vinohrady']\n",
      "['Petrzilkova', 'Praha 5 - Stodulky']\n",
      "['Jetelova', 'Praha 10 - Zabehlice']\n",
      "['Chodska', 'Praha 2 - Vinohrady']\n",
      "['Italska', 'Praha 2 - Vinohrady', 'okres Praha']\n",
      "['Slavikova', 'Praha 2 - Vinohrady', 'okres Praha']\n"
     ]
    }
   ],
   "source": [
    "b.get_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
