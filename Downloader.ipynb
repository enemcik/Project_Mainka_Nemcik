{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from gensim.utils import deaccent\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "#geocoding\n",
    "from bokeh.io import output_notebook, show, output_file\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import GeoJSONDataSource, LinearColorMapper, ColorBar, NumeralTickFormatter\n",
    "from bokeh.palettes import brewer\n",
    "\n",
    "from bokeh.io.doc import curdoc\n",
    "from bokeh.models import Slider, HoverTool, Select\n",
    "from bokeh.layouts import widgetbox, row, column\n",
    "import geopy\n",
    "import geopandas as gpd\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "#visualisation\n",
    "import folium\n",
    "import folium.plugins as plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_soups(links, name):\n",
    "        '''\n",
    "        This function iterates over all search pages, converts them into a BeautifulSoup object and stores them in a txt file as \n",
    "        strings outside of this script. BREAKHERE is used to distinguish between objects. \n",
    "        '''\n",
    "        soups = []\n",
    "        for link in links:\n",
    "            sleep(random.uniform(0.5, 2))\n",
    "            request = requests.get(link)\n",
    "            request.encoding='UTF-8'\n",
    "            soups.append(BeautifulSoup(request.text,'lxml'))\n",
    "        with open(name, 'w') as f:\n",
    "            for s in soups:\n",
    "                f.write(str(deaccent(s).encode(\"utf-8\")) + 'BREAKHERE')\n",
    "            f.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data from websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileDir = os.path.dirname(os.path.realpath('__file__'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloaderBezRealitky(): #error prone, need to correct\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        For the bezrealitky search, you need to iterate over search pages. Self.page_bezrealitky stores the maximum amount\n",
    "        of pages and then via self.link a list of all pages from search is created in self.hrefs_bezrealitky.\n",
    "        '''\n",
    "        self.link = 'https://www.bezrealitky.cz/vypis/nabidka-pronajem/byt/praha?_token=pr1lf-vKwDFfmFbICiz2PfC-Zdwq-2JolXi4MeMHsrw&page=1'\n",
    "        self.request = requests.get(self.link)\n",
    "        self.request.encoding='UTF-8'\n",
    "        self.soup = BeautifulSoup(self.request.text,'lxml')\n",
    "        self.page_bezrealitky = int(self.soup.findAll('a',{'class':'page-link pagination__page'})[-2].text)\n",
    "        self.hrefs_bezrealitky = ['https://www.bezrealitky.cz/vypis/nabidka-pronajem/byt/praha?_token=pr1lf-vKwDFfmFbICiz2PfC-Zdwq-2JolXi4MeMHsrw&page=' \n",
    "                                  + str(i) for i in range(1,self.page_bezrealitky)]\n",
    "        self.soups = []\n",
    "        self.counter = counter\n",
    "\n",
    "    def get_data(self):\n",
    "        '''\n",
    "        Main method to obtain and transform the data. HTMLs are read from the txt file and stored in a list (soup_list) \n",
    "        within this script. Next, the method iterates over the list, converts the strings in txt file into a BeautifulSoup\n",
    "        object and parses the html for relevant data. At the end, a nested dictionary (dicts) is created and stored\n",
    "        as a json file outside of this script.\n",
    "        '''\n",
    "        with open(fileDir + '\\\\Data\\\\bezrealitky_links.txt', 'r') as f:\n",
    "            content = f.read()\n",
    "        soup_list = content.split('BREAKHERE')\n",
    "        dicts = {}\n",
    "        counter = 0\n",
    "        for soup in soup_list:\n",
    "            descrips = [] #empty list for apartment values\n",
    "            values = [] #empty list for apartment prices\n",
    "            vals = BeautifulSoup(soup,'lxml').findAll('strong', {'class':'product__value'}) #parsing for apartment values\n",
    "            ##vals = soup.findAll('strong', {'class':'product__value'})\n",
    "            for vl in vals:\n",
    "                values.append(vl.text.strip())\n",
    "            #img = soup.findAll('img')\n",
    "            img = BeautifulSoup(soup,'lxml').findAll('img') #parsing for apartment info (street, city, size..)\n",
    "            for i in img:\n",
    "                if 'Pronajem' and 'obr. c. 1' in i['alt']: #info present at all pictures, let's take info from the first one\n",
    "                        info = i['alt'].split(',')[0:4] #info separated by comma, split into a list\n",
    "                        if 'Praha' == info[-1].strip(): #if street non present, insert a NaN instead\n",
    "                            info.insert(2, 'NaN')\n",
    "                            del info[-1]\n",
    "                            m = info[1].split(' ')\n",
    "                            info[1] = m[1]\n",
    "                            descrips.append(info)\n",
    "                        else:\n",
    "                            m = info[1].split(' ')\n",
    "                            info[1] = m[1]\n",
    "                            descrips.append(info)\n",
    "            count = 0\n",
    "            for pp in values: #append apartment prices to info about apartments in list descrips\n",
    "                try:\n",
    "                    descrips[count].append(pp)\n",
    "                    descrips[count][0] = descrips[count][0][-4:].strip()\n",
    "                    count += 1\n",
    "                except IndexError:\n",
    "                    count += 1\n",
    "                    continue\n",
    "            for item in descrips:\n",
    "                try:\n",
    "                    if '+' in item[4]: #prices often written as '19000 Kč + 4000Kč' so we need to split it\n",
    "                        prices = item.pop(4).split('+')\n",
    "                        item.append(re.sub(\"[^0-9]\", \"\", prices[0])) #keep only numeric characters, i.e. price\n",
    "                        item.append(re.sub(\"[^0-9]\", \"\", prices[1]))\n",
    "                    else:\n",
    "                        prices = [item.pop(4), '0'] #if only '19000 Kč', insert 0 as price for utilities not specified\n",
    "                        item.append(re.sub(\"[^0-9]\", \"\", prices[0]))\n",
    "                        item.append(re.sub(\"[^0-9]\", \"\", prices[1]))\n",
    "                except IndexError:\n",
    "                    continue\n",
    "            for item in descrips: #store apartment info, price into a dictionary and index by counter\n",
    "                try:\n",
    "                    dict = {}\n",
    "                    dict['Size'] = item[0]\n",
    "                    dict['m2'] = re.sub(\"[^0-9]\", \"\", item[1]) #keep only size, i.e. numeric characters\n",
    "                    dict['Street'] = deaccent(item[2]) #deaccent to provent potential errors\n",
    "                    dict['District'] = deaccent(item[3])\n",
    "                    dict['Base Price'] = int(item[4])\n",
    "                    dict['Utilities Price'] = int(item[5])\n",
    "                    dict['Total Price'] = int(item[4]) + int(item[5])\n",
    "                    dict['Source'] = 'bezrealitky.cz'\n",
    "                    dicts[self.counter] = dict\n",
    "                    self.counter += 1\n",
    "                except IndexError:\n",
    "                    #counter +=1\n",
    "                    continue\n",
    "            print('Done loop number ' + str(self.counter) + '. Printing descrips.')\n",
    "        with open(fileDir + '\\\\Data\\\\bezrealitky.json', 'w') as write_file: #store data into a json file\n",
    "            json.dump(dicts, write_file, indent = 4)\n",
    "\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DownloaderBezRealitky()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_soups(a.hrefs_bezrealitky[0:2], fileDir + '\\\\Data\\\\bezrealitky_links.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = a.counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloaderReality():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        For the reality search, you need to iterate over search pages. Self.page_reality stores the maximum amount\n",
    "        of pages and then via self.link a list of all pages from search is created in self.hrefs_reality.\n",
    "        '''\n",
    "        self.link = 'https://reality.idnes.cz/s/pronajem/byty/praha/?page=1'\n",
    "        self.request = requests.get(self.link)\n",
    "        self.request.encoding='UTF-8'\n",
    "        self.soup = BeautifulSoup(self.request.text,'lxml')\n",
    "        self.page_reality = int(self.soup.findAll('a',{'class':'btn btn--border paging__item'})[-1].text) - 1\n",
    "        self.hrefs_reality = ['https://reality.idnes.cz/s/pronajem/byty/praha/?page=' \n",
    "                                  + str(i) for i in range(1,self.page_reality)]\n",
    "        self.soups = []\n",
    "        self.counter = counter\n",
    "        \n",
    "    def get_data(self):\n",
    "        '''\n",
    "        Main method to obtain and transform the data. HTMLs are read from the txt file and stored in a list (soup_list) \n",
    "        within this script. Next, the method iterates over the list, converts the strings in txt file into a BeautifulSoup\n",
    "        object and parses the html for relevant data. At the end, a nested dictionary (dicts) is created and stored\n",
    "        as a json file outside of this script.\n",
    "        '''\n",
    "        with open(fileDir + '\\\\Data\\\\reality_idnes_links.txt', 'r') as f:\n",
    "            content = f.read()\n",
    "        soup_list = content.split('BREAKHERE')\n",
    "        dicts = {}\n",
    "        counter = 0\n",
    "        for soup in soup_list:\n",
    "            descrips = [] #empty list for apartment values\n",
    "            values = [] #empty list for apartment prices\n",
    "            info_size = []\n",
    "            apartments = []\n",
    "            vals = BeautifulSoup(soup,'lxml').findAll('p', {'class':'c-list-products__price'}) #parsing for apartment values\n",
    "            for vl in vals: #adding values\n",
    "                values.append(re.sub(\"[^0-9]\", \"\",vl.find('strong').text))\n",
    "                \n",
    "            locs = BeautifulSoup(soup,'lxml').findAll('p', {'class':'c-list-products__info'})\n",
    "            for i in locs: #adding location\n",
    "                if 'Komercni sdeleni' in i.text:\n",
    "                    continue\n",
    "                else:\n",
    "                    temp_info = str(i.text)\n",
    "                    temp_info = re.sub(r'^(?:\\\\n)+','', temp_info).strip()[:-2]\n",
    "                    temp_info = temp_info.strip().split(',')\n",
    "                    temp_info = [i.strip() for i in temp_info]\n",
    "                    if len(temp_info) == 1:\n",
    "                        temp_info.append(temp_info[0])\n",
    "                        temp_info[0] = 'NaN'\n",
    "                    if len(temp_info) == 3:\n",
    "                        del temp_info[2]\n",
    "                    descrips.append(temp_info)\n",
    "                    \n",
    "            sizes = BeautifulSoup(soup,'lxml').findAll('h2', {'class':'c-list-products__title'})\n",
    "            for s in sizes: #adding size and m2\n",
    "                try:\n",
    "                    item = s.text.split('bytu')[1].strip()[:-2]\n",
    "                    temp = item.split(',')\n",
    "                    temp[1] = temp[1][:-10].strip()\n",
    "                    info_size.append(temp)\n",
    "                except IndexError:\n",
    "                    continue\n",
    "            \n",
    "            for apart in range(0,len(info_size)):\n",
    "                apartments.append(info_size[apart] + descrips[apart] + [values[apart]])\n",
    "                \n",
    "            for item in apartments: #store apartment info, price into a dictionary and index by counter\n",
    "                try:\n",
    "                    dict = {}\n",
    "                    dict['Size'] = item[0]\n",
    "                    dict['m2'] = item[1]\n",
    "                    dict['Street'] = deaccent(item[2]) #deaccent to provent potential errors\n",
    "                    dict['District'] = deaccent(item[3])\n",
    "                    dict['Base Price'] = int(item[4])\n",
    "                    dict['Utilities Price'] = 0\n",
    "                    dict['Total Price'] = int(item[4])\n",
    "                    dict['Source'] = 'reality.idnes.cz'\n",
    "                    dicts[self.counter] = dict\n",
    "                    self.counter +=1\n",
    "                except ValueError:\n",
    "                    #counter += 1\n",
    "                    continue\n",
    "            print('Done loop number ' + str(self.counter) + '. Printing apartments.')\n",
    "        with open(fileDir + '\\\\Data\\\\idnes_reality.json', 'w') as write_file: #store data into a json file\n",
    "            json.dump(dicts, write_file, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = DownloaderReality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_soups(b.hrefs_reality[0:2], fileDir + '\\\\Data\\\\reality_idnes_links.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloaderCeskeReality():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        For the reality search, you need to iterate over search pages. Self.page_reality stores the maximum amount\n",
    "        of pages and then via self.link a list of all pages from search is created in self.hrefs_reality.\n",
    "        '''\n",
    "        self.link = 'https://www.ceskereality.cz/pronajem/byty/praha/?strana=2'\n",
    "\n",
    "        self.request = requests.get(self.link)\n",
    "        self.request.encoding='UTF-8'\n",
    "        self.soup = BeautifulSoup(self.request.text, 'html.parser')\n",
    "        \n",
    "        self.page_ceskereality = int([page.text for page in self.soup.findAll('ul',{'class':'pages'})[0]][-2]) - 1\n",
    "        self.hrefs_reality = ['https://www.ceskereality.cz/pronajem/byty/praha/?strana=' \n",
    "                        + str(i) for i in range(1,self.page_ceskereality)]\n",
    "        self.soups = []\n",
    "        \n",
    "    def get_soups(self):\n",
    "        '''\n",
    "        This method iterates over all search pages, converts them into a BeautifulSoup object and stores them in a txt file as \n",
    "        strings outside of this script. BREAKHERE is used to distinguish between objects. \n",
    "        '''\n",
    "        for link in self.hrefs_reality[0:3]:\n",
    "            sleep(random.uniform(0.5, 2))\n",
    "            self.link = link\n",
    "            self.request = requests.get(self.link)\n",
    "            self.request.encoding='utf-8'\n",
    "            self.soups.append(BeautifulSoup(self.request.text,'html.parser'))\n",
    "            print('Page saved.')\n",
    "            print(self.soups)\n",
    "        with open('ceske_reality_links.txt', 'w') as f:\n",
    "            for s in self.soups:\n",
    "                f.write(str(deaccent(s).encode(\"utf-8\")) + 'BREAKHERE')\n",
    "            f.close\n",
    "    \n",
    "    def get_data(self):\n",
    "        '''\n",
    "        Main method to obtain and transform the data. HTMLs are read from the txt file and stored in a list (soup_list) \n",
    "        within this script. Next, the method iterates over the list, converts the strings in txt file into a BeautifulSoup\n",
    "        object and parses the html for relevant data. At the end, a nested dictionary (dicts) is created and stored\n",
    "        as a json file outside of this script.\n",
    "        '''\n",
    "        with open('ceske_reality_links.txt', 'r') as f:\n",
    "            content = f.read()\n",
    "        soup_list = content.split('BREAKHERE')\n",
    "        dicts = {}\n",
    "        #counter = 0\n",
    "        for soup in soup_list[0:1]:\n",
    "            descrips = [] #empty list for apartment values\n",
    "            values = [] #empty list for apartment prices\n",
    "            info_size = []\n",
    "            apartments = []\n",
    "            vals = BeautifulSoup(soup,'lxml').findAll('div', {'class':'cena'}) #parsing for apartment values\n",
    "            for value in vals:\n",
    "                values.append(re.sub(\"[^0-9]\", \"\",value.text.split(',')[0]))\n",
    "            locs = BeautifulSoup(soup,'lxml').findAll('div', {'class':'div_nemovitost suda'})\n",
    "            for item in locs:\n",
    "                print(item.text)\n",
    "            #print(locs)\n",
    "\n",
    "            '''\n",
    "            for item in apartments: #store apartment info, price into a dictionary and index by counter\n",
    "                try:\n",
    "                    dict = {}\n",
    "                    dict['Size'] = item[0]\n",
    "                    dict['m2'] = item[1]\n",
    "                    dict['Street'] = deaccent(item[2]) #deaccent to provent potential errors\n",
    "                    dict['District'] = deaccent(item[3])\n",
    "                    dict['Base Price'] = int(item[4])\n",
    "                    dict['Utilities Price'] = 0\n",
    "                    dict['Total Price'] = int(item[4]) \n",
    "                    dicts[counter] = dict\n",
    "                    counter +=1\n",
    "                except ValueError:\n",
    "                    counter += 1\n",
    "                    continue\n",
    "            print('Done loop number ' + str(counter) + '. Printing apartments.')\n",
    "        with open('idnes_reality.json', 'w') as write_file: #store data into a json file\n",
    "            json.dump(dicts, write_file, indent = 4)      \n",
    "            '''          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = DownloaderCeskeReality()\n",
    "c.get_soups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_soups(c.hrefs_reality[0:2], 'blah.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the fetched data into one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_dict = []\n",
    "data = {}\n",
    "def data_combine(*args):\n",
    "    #input example - 'idnes_reality.json', 'bezrealitky.json'\n",
    "    for arg in args:\n",
    "        with open(fileDir + '\\\\Data\\\\' + arg) as json_file:\n",
    "            file_ = json.load(json_file)\n",
    "            big_dict.append(file_)\n",
    "    for dt in big_dict:\n",
    "        data.update(dt)\n",
    "\n",
    "data_combine('bezrealitky.json', 'idnes_reality.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(data).T\n",
    "dataframe = dataframe.replace('NaN', '', regex=True)\n",
    "dataframe['Address'] = dataframe['Street'] + ',' + dataframe['District'] + ',' + 'Praha'\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation - Apartment locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locator = geopy.Nominatim(user_agent='myGeocoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocode = RateLimiter(locator.geocode) #, min_delay_seconds=1)\n",
    "\n",
    "dataframe['location'] = dataframe['Address'].apply(geocode)\n",
    "\n",
    "dataframe['point'] = dataframe['location'].apply(lambda loc: tuple(loc.point) if loc else None)\n",
    "\n",
    "dataframe[['latitude', 'longitude', 'altitude']] = pd.DataFrame(dataframe['point'].tolist(), index=dataframe.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['point'] = [tuple(list(x)[0:2]) for x in dataframe['point']]\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folium_map = folium.Map(location=[50.08804, 14.42076],\n",
    "                        zoom_start=12,\n",
    "                        tiles='cartodbpositron')\n",
    "\n",
    "plugins.FastMarkerCluster(data=list(zip(dataframe['latitude'].values, dataframe['longitude'].values))).add_to(folium_map)\n",
    "\n",
    "popup = folium.Popup(dataframe['Base Price'])\n",
    "popup.add_to(folium_map)                          \n",
    "\n",
    "folium_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisations - Neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Praha.json') as data: #could be automated to get the data?\n",
    "    hoods = json.loads(data.read())\n",
    "    \n",
    "gdf = gpd.GeoDataFrame.from_features(hoods[\"features\"])\n",
    "print(gdf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_indv = gpd.GeoDataFrame(dataframe, geometry = gpd.points_from_xy(dataframe.longitude, dataframe.latitude))\n",
    "print(gdf_indv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = gpd.sjoin(gdf_indv, gdf, op='within') \n",
    "df_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
