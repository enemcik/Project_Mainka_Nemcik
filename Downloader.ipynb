{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from gensim.utils import deaccent\n",
    "import random\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geocoding\n",
    "from bokeh.io import output_notebook, show, output_file\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import GeoJSONDataSource, LinearColorMapper, ColorBar, NumeralTickFormatter\n",
    "from bokeh.palettes import brewer\n",
    "\n",
    "from bokeh.io.doc import curdoc\n",
    "from bokeh.models import Slider, HoverTool, Select\n",
    "from bokeh.layouts import widgetbox, row, column\n",
    "import geopy\n",
    "import geopandas as gpd\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualisation\n",
    "import folium\n",
    "import folium.plugins as plugins\n",
    "from folium.plugins import MarkerCluster\n",
    "#from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_soups(links, name):\n",
    "        '''\n",
    "        This function iterates over all search pages, converts them into a BeautifulSoup object and stores them in a JSON file as \n",
    "        outside of this script. The keys of the dictoniary distinguish here between the different objects/HTML-pages. \n",
    "        '''\n",
    "        count = 0\n",
    "        dict_ = {}\n",
    "        soups = []\n",
    "        for link in links:\n",
    "            sleep(random.uniform(0.5, 2))\n",
    "            request = requests.get(link)\n",
    "            request.encoding='UTF-8'\n",
    "            soups.append(BeautifulSoup(request.text,'lxml'))\n",
    "        for soup in soups:\n",
    "            dict_[count] = str(deaccent(soup).encode(\"utf-8\"))\n",
    "            count += 1\n",
    "        with open(name, 'w') as write_file:\n",
    "            json.dump(dict_, write_file, indent = 4)\n",
    "        \n",
    "        '''\n",
    "        with open(name, 'w') as f:\n",
    "            for s in soups:\n",
    "                f.write(str(deaccent(s).encode(\"utf-8\")))\n",
    "            f.close\n",
    "            \n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data from websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "'''Create a new Folder \"Data\" in the current working directory to store & access the data files which will be produced throughout this script'''\n",
    "newfolder = r'Data' \n",
    "if not os.path.exists(newfolder): #if already exists will not be created again\n",
    "    os.makedirs(newfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloaderBezRealitky(): #error prone, need to correct\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        For the bezrealitky search, you need to iterate over search pages. Self.page_bezrealitky stores the maximum amount\n",
    "        of pages and then via self.link a list of all pages from search is created in self.hrefs_bezrealitky.\n",
    "        '''\n",
    "        self.link = 'https://www.bezrealitky.cz/vypis/nabidka-pronajem/byt/praha?_token=pr1lf-vKwDFfmFbICiz2PfC-Zdwq-2JolXi4MeMHsrw&page=1'\n",
    "        self.request = requests.get(self.link)\n",
    "        self.request.encoding='UTF-8'\n",
    "        self.soup = BeautifulSoup(self.request.text,'lxml')\n",
    "        self.page_bezrealitky = int(self.soup.findAll('a',{'class':'page-link pagination__page'})[-2].text)\n",
    "        self.hrefs_bezrealitky = ['https://www.bezrealitky.cz/vypis/nabidka-pronajem/byt/praha?_token=pr1lf-vKwDFfmFbICiz2PfC-Zdwq-2JolXi4MeMHsrw&page=' \n",
    "                                  + str(i) for i in range(1,self.page_bezrealitky)]\n",
    "        self.soups = []\n",
    "        self.counter = counter\n",
    "\n",
    "    def get_data(self):\n",
    "        '''\n",
    "        Main method to obtain and transform the data. HTMLs are read from the JSON file and stored in a list (soup_list) \n",
    "        within this script. Next, the method iterates over the list, converts the strings in the list into a BeautifulSoup\n",
    "        object and parses the html for relevant data. At the end, a nested dictionary (dicts) is created and stored\n",
    "        as a json file outside of this script.\n",
    "        '''\n",
    "        with open(fileDir + '\\\\Data\\\\bezrealitky_links.json', 'r', encoding='utf-8') as f:\n",
    "            content = json.load(f)\n",
    "        soup_list = list(content.values())\n",
    "        dicts = {}\n",
    "        counter = 0\n",
    "        for soup in soup_list:\n",
    "            descrips = [] #empty list for apartment values\n",
    "            values = [] #empty list for apartment prices\n",
    "            vals = BeautifulSoup(soup,'lxml').findAll('strong', {'class':'product__value'}) #parsing for apartment values\n",
    "            ##vals = soup.findAll('strong', {'class':'product__value'})\n",
    "            for vl in vals:\n",
    "                values.append(vl.text.strip())\n",
    "            #img = soup.findAll('img')\n",
    "            img = BeautifulSoup(soup,'lxml').findAll('img') #parsing for apartment info (street, city, size..)\n",
    "            for i in img:\n",
    "                if 'Pronajem' and 'obr. c. 1' in i['alt']: #info present at all pictures, let's take info from the first one\n",
    "                        info = i['alt'].split(',')[0:4] #info separated by comma, split into a list\n",
    "                        if 'Praha' == info[-1].strip(): #if street non present, insert a NaN instead\n",
    "                            info.insert(2, 'NaN')\n",
    "                            del info[-1]\n",
    "                            m = info[1].split(' ')\n",
    "                            info[1] = m[1]\n",
    "                            descrips.append(info)\n",
    "                        else:\n",
    "                            m = info[1].split(' ')\n",
    "                            info[1] = m[1]\n",
    "                            descrips.append(info)\n",
    "            count = 0\n",
    "            for pp in values: #append apartment prices to info about apartments in list descrips\n",
    "                try:\n",
    "                    descrips[count].append(pp)\n",
    "                    descrips[count][0] = descrips[count][0][-4:].strip()\n",
    "                    count += 1\n",
    "                except IndexError:\n",
    "                    count += 1\n",
    "                    continue\n",
    "            for item in descrips:\n",
    "                try:\n",
    "                    if '+' in item[4]: #prices often written as '19000 Kč + 4000Kč' so we need to split it\n",
    "                        prices = item.pop(4).split('+')\n",
    "                        item.append(re.sub(\"[^0-9]\", \"\", prices[0])) #keep only numeric characters, i.e. price\n",
    "                        item.append(re.sub(\"[^0-9]\", \"\", prices[1]))\n",
    "                    else:\n",
    "                        prices = [item.pop(4), '0'] #if only '19000 Kč', insert 0 as price for utilities not specified\n",
    "                        item.append(re.sub(\"[^0-9]\", \"\", prices[0]))\n",
    "                        item.append(re.sub(\"[^0-9]\", \"\", prices[1]))\n",
    "                except IndexError:\n",
    "                    continue\n",
    "            for item in descrips: #store apartment info, price into a dictionary and index by counter\n",
    "                try:\n",
    "                    dict = {}\n",
    "                    dict['Size'] = item[0]\n",
    "                    dict['m2'] = re.sub(\"[^0-9]\", \"\", item[1]) #keep only size, i.e. numeric characters\n",
    "                    dict['Street'] = deaccent(item[2]) #deaccent to provent potential errors\n",
    "                    dict['District'] = deaccent(item[3])\n",
    "                    dict['Base Price'] = int(item[4])\n",
    "                    dict['Utilities Price'] = int(item[5])\n",
    "                    dict['Total Price'] = int(item[4]) + int(item[5])\n",
    "                    dict['Source'] = 'bezrealitky.cz'\n",
    "                    dicts[self.counter] = dict\n",
    "                    self.counter += 1\n",
    "                except IndexError:\n",
    "                    #counter +=1\n",
    "                    continue\n",
    "            print('Done loop number ' + str(self.counter) + '. Printing descrips.')\n",
    "        with open(fileDir + '\\\\Data\\\\bezrealitky.json', 'w') as write_file: #store data into a json file\n",
    "            json.dump(dicts, write_file, indent = 4)\n",
    "\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DownloaderBezRealitky()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_soups(a.hrefs_bezrealitky, fileDir + '\\\\Data\\\\bezrealitky_links.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = a.counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloaderReality():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        For the reality search, you need to iterate over search pages. Self.page_reality stores the maximum amount\n",
    "        of pages and then via self.link a list of all pages from search is created in self.hrefs_reality.\n",
    "        '''\n",
    "        self.link = 'https://reality.idnes.cz/s/pronajem/byty/praha/?page=1'\n",
    "        self.request = requests.get(self.link)\n",
    "        self.request.encoding='UTF-8'\n",
    "        self.soup = BeautifulSoup(self.request.text,'lxml')\n",
    "        self.page_reality = int(self.soup.findAll('a',{'class':'btn btn--border paging__item'})[-1].text) - 1\n",
    "        self.hrefs_reality = ['https://reality.idnes.cz/s/pronajem/byty/praha/?page=' \n",
    "                                  + str(i) for i in range(1,self.page_reality)]\n",
    "        self.soups = []\n",
    "        self.counter = counter\n",
    "        \n",
    "    def get_data(self):\n",
    "        '''\n",
    "        Main method to obtain and transform the data. HTMLs are read from the JSON file and stored in a list (soup_list) \n",
    "        within this script. Next, the method iterates over the list, converts the strings in txt file into a BeautifulSoup\n",
    "        object and parses the html for relevant data. At the end, a nested dictionary (dicts) is created and stored\n",
    "        as a json file outside of this script.\n",
    "        '''\n",
    "        '''\n",
    "        with open(fileDir + '\\\\Data\\\\reality_idnes_links.txt', 'r') as f:\n",
    "            content = f.read()\n",
    "        soup_list = content.split('BREAKHERE')\n",
    "        '''\n",
    "        with open(fileDir + '\\\\Data\\\\reality_idnes_links.json', 'r', encoding='utf-8') as f:\n",
    "            content = json.load(f)\n",
    "        soup_list = list(content.values())\n",
    "        dicts = {}\n",
    "        counter = 0\n",
    "        for soup in soup_list:\n",
    "            descrips = [] #empty list for apartment values\n",
    "            values = [] #empty list for apartment prices\n",
    "            info_size = []\n",
    "            apartments = []\n",
    "            vals = BeautifulSoup(soup,'lxml').findAll('p', {'class':'c-list-products__price'}) #parsing for apartment values\n",
    "            for vl in vals: #adding values\n",
    "                values.append(re.sub(\"[^0-9]\", \"\",vl.find('strong').text))\n",
    "                \n",
    "            locs = BeautifulSoup(soup,'lxml').findAll('p', {'class':'c-list-products__info'})\n",
    "            for i in locs: #adding location\n",
    "                if 'Komercni sdeleni' in i.text:\n",
    "                    continue\n",
    "                else:\n",
    "                    temp_info = str(i.text)\n",
    "                    temp_info = re.sub(r'^(?:\\\\n)+','', temp_info).strip()[:-2]\n",
    "                    temp_info = temp_info.strip().split(',')\n",
    "                    temp_info = [i.strip() for i in temp_info]\n",
    "                    if len(temp_info) == 1:\n",
    "                        temp_info.append(temp_info[0])\n",
    "                        temp_info[0] = 'NaN'\n",
    "                    if len(temp_info) == 3:\n",
    "                        del temp_info[2]\n",
    "                    descrips.append(temp_info)\n",
    "                    \n",
    "            sizes = BeautifulSoup(soup,'lxml').findAll('h2', {'class':'c-list-products__title'})\n",
    "            for s in sizes: #adding size and m2\n",
    "                try:\n",
    "                    item = s.text.split('bytu')[1].strip()[:-2]\n",
    "                    temp = item.split(',')\n",
    "                    temp[1] = temp[1][:-10].strip()\n",
    "                    info_size.append(temp)\n",
    "                except IndexError:\n",
    "                    continue\n",
    "            \n",
    "            for apart in range(0,len(info_size)):\n",
    "                apartments.append(info_size[apart] + descrips[apart] + [values[apart]])\n",
    "                \n",
    "            for item in apartments: #store apartment info, price into a dictionary and index by counter\n",
    "                try:\n",
    "                    dict = {}\n",
    "                    dict['Size'] = item[0]\n",
    "                    dict['m2'] = item[1]\n",
    "                    dict['Street'] = deaccent(item[2]) #deaccent to provent potential errors\n",
    "                    dict['District'] = deaccent(item[3])\n",
    "                    dict['Base Price'] = int(item[4])\n",
    "                    dict['Utilities Price'] = 0\n",
    "                    dict['Total Price'] = int(item[4])\n",
    "                    dict['Source'] = 'reality.idnes.cz'\n",
    "                    dicts[self.counter] = dict\n",
    "                    self.counter +=1\n",
    "                except ValueError:\n",
    "                    #counter += 1\n",
    "                    continue\n",
    "            print('Done loop number ' + str(self.counter) + '. Printing apartments.')\n",
    "        with open(fileDir + '\\\\Data\\\\idnes_reality.json', 'w') as write_file: #store data into a json file\n",
    "            json.dump(dicts, write_file, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = DownloaderReality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_soups(b.hrefs_reality, fileDir + '\\\\Data\\\\reality_idnes_links.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloaderCeskeReality():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        For the reality search, you need to iterate over search pages. Self.page_reality stores the maximum amount\n",
    "        of pages and then via self.link a list of all pages from search is created in self.hrefs_reality.\n",
    "        '''\n",
    "        self.link = 'https://www.ceskereality.cz/pronajem/byty/praha/?strana=2'\n",
    "\n",
    "        self.request = requests.get(self.link)\n",
    "        self.request.encoding='UTF-8'\n",
    "        self.soup = BeautifulSoup(self.request.text, 'html.parser')\n",
    "        \n",
    "        self.page_ceskereality = int([page.text for page in self.soup.findAll('ul',{'class':'pages'})[0]][-2]) - 1\n",
    "        self.hrefs_reality = ['https://www.ceskereality.cz/pronajem/byty/praha/?strana=' \n",
    "                        + str(i) for i in range(1,self.page_ceskereality)]\n",
    "        self.soups = []\n",
    "        \n",
    "    def get_soups(self):\n",
    "        '''\n",
    "        This method iterates over all search pages, converts them into a BeautifulSoup object and stores them in a txt file as \n",
    "        strings outside of this script. BREAKHERE is used to distinguish between objects. \n",
    "        '''\n",
    "        for link in self.hrefs_reality[0:3]:\n",
    "            sleep(random.uniform(0.5, 2))\n",
    "            self.link = link\n",
    "            self.request = requests.get(self.link)\n",
    "            self.request.encoding='utf-8'\n",
    "            self.soups.append(BeautifulSoup(self.request.text,'html.parser'))\n",
    "            print('Page saved.')\n",
    "            print(self.soups)\n",
    "        with open('ceske_reality_links.txt', 'w') as f:\n",
    "            for s in self.soups:\n",
    "                f.write(str(deaccent(s).encode(\"utf-8\")) + 'BREAKHERE')\n",
    "            f.close\n",
    "    \n",
    "    def get_data(self):\n",
    "        '''\n",
    "        Main method to obtain and transform the data. HTMLs are read from the txt file and stored in a list (soup_list) \n",
    "        within this script. Next, the method iterates over the list, converts the strings in txt file into a BeautifulSoup\n",
    "        object and parses the html for relevant data. At the end, a nested dictionary (dicts) is created and stored\n",
    "        as a json file outside of this script.\n",
    "        '''\n",
    "        with open('ceske_reality_links.txt', 'r') as f:\n",
    "            content = f.read()\n",
    "        soup_list = content.split('BREAKHERE')\n",
    "        dicts = {}\n",
    "        #counter = 0\n",
    "        for soup in soup_list[0:1]:\n",
    "            descrips = [] #empty list for apartment values\n",
    "            values = [] #empty list for apartment prices\n",
    "            info_size = []\n",
    "            apartments = []\n",
    "            vals = BeautifulSoup(soup,'lxml').findAll('div', {'class':'cena'}) #parsing for apartment values\n",
    "            for value in vals:\n",
    "                values.append(re.sub(\"[^0-9]\", \"\",value.text.split(',')[0]))\n",
    "            locs = BeautifulSoup(soup,'lxml').findAll('div', {'class':'div_nemovitost suda'})\n",
    "            for item in locs:\n",
    "                print(item.text)\n",
    "            #print(locs)\n",
    "\n",
    "            '''\n",
    "            for item in apartments: #store apartment info, price into a dictionary and index by counter\n",
    "                try:\n",
    "                    dict = {}\n",
    "                    dict['Size'] = item[0]\n",
    "                    dict['m2'] = item[1]\n",
    "                    dict['Street'] = deaccent(item[2]) #deaccent to provent potential errors\n",
    "                    dict['District'] = deaccent(item[3])\n",
    "                    dict['Base Price'] = int(item[4])\n",
    "                    dict['Utilities Price'] = 0\n",
    "                    dict['Total Price'] = int(item[4]) \n",
    "                    dicts[counter] = dict\n",
    "                    counter +=1\n",
    "                except ValueError:\n",
    "                    counter += 1\n",
    "                    continue\n",
    "            print('Done loop number ' + str(counter) + '. Printing apartments.')\n",
    "        with open('idnes_reality.json', 'w') as write_file: #store data into a json file\n",
    "            json.dump(dicts, write_file, indent = 4)      \n",
    "            '''          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = DownloaderCeskeReality()\n",
    "c.get_soups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_soups(c.hrefs_reality[0:2], 'blah.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the fetched data into one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_dict = []\n",
    "data = {}\n",
    "def data_combine(*args):\n",
    "    #input example - 'idnes_reality.json', 'bezrealitky.json'\n",
    "    for arg in args:\n",
    "        with open(fileDir + '\\\\Data\\\\' + arg) as json_file:\n",
    "            file_ = json.load(json_file)\n",
    "            big_dict.append(file_)\n",
    "    for dt in big_dict:\n",
    "        data.update(dt)\n",
    "\n",
    "data_combine('bezrealitky.json', 'idnes_reality.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(data_file):\n",
    "    '''\n",
    "    The clean_dataframe function takes a data file (here a dictoniary) as an input and returns a pandas dataframe, which is cleaned up and ready to use. \n",
    "    In particular, NaN values are replaced with nothing, white spaces before and after strings in the columns which have strings are removed \n",
    "    (which is important for the duplicate search), Rows which are duplicates (ergo same flat) are removed, the removal is executed based on the columns\n",
    "    Size, m2, Street and Total Price as it is highly likely that in case each of these values is identical the flat is identical and a new column 'Address'\n",
    "    is created which is necessary for geocoding.\n",
    "    '''\n",
    "    df = pd.DataFrame(data_file).T\n",
    "    df = df.replace('NaN', '', regex=True)\n",
    "    for name in ['Size','Street','District']: #strips all white spaces before and after strings\n",
    "        df[name]=df[name].str.strip()\n",
    "    print('Number of (removed) duplicates: ' + str(df.duplicated(['Size', 'm2', 'Street', 'Total Price']).sum()))\n",
    "    df = df.drop_duplicates(['Size', 'm2', 'Street', 'Total Price'], ignore_index=True) #drops duplicates \n",
    "    df['Address'] = df['Street'] + ',' + df['District'] + ',' + 'Praha' #creating address column for geocoding\n",
    "    return df\n",
    "\n",
    "dataframe = clean_dataframe(data)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locator = geopy.Nominatim(user_agent='myGeocoder')\n",
    "#locator = geopy.GoogleV3(api_key='AIzaSyDgWSTfwvVV3AELge6lJCw8hT0T4TwejYc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the GPS addresses\n",
    "geocode = RateLimiter(locator.geocode, min_delay_seconds=1) #this process takes about 2,5 hours\n",
    "\n",
    "'''We use Nominatim, an open source geocoding provider to retrive the locations (latitude, longitude, altitude) for our apartments. \n",
    "For this we provide Nominatim with the Addresses of the aparments.'''\n",
    "\n",
    "dataframe['location'] = dataframe['Address'].apply(geocode)\n",
    "\n",
    "dataframe['point'] = dataframe['location'].apply(lambda loc: tuple(loc.point) if loc else None)\n",
    "\n",
    "dataframe[['latitude', 'longitude', 'altitude']] = pd.DataFrame(dataframe['point'].tolist(), index=dataframe.index)\n",
    "\n",
    "dataframe = dataframe.dropna()\n",
    "dataframe.to_pickle(fileDir + '\\\\Data\\\\' +'geo_df.pkl', protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_pickle(fileDir + '\\\\Data\\\\' +'geo_df.pkl') #load from here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation - Apartment locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create a new map with folium, the map doesnt contain any datapoints yet but is intialized at the mean latitude & longitude\n",
    "point in our dataset'''\n",
    "\n",
    "new_map = folium.Map(location=[dataframe['latitude'].mean(), dataframe['longitude'].mean()], \n",
    "                        zoom_start=12,\n",
    "                        tiles='cartodbpositron')\n",
    "\n",
    "'''Add data points = flats (markers) to our map. For each observation (row) of the dataset we read the latitude & longitude\n",
    "to create an icon which will be a display for the flat on the map. Furthermore we add a pop up text with basic information\n",
    "to each icon. We create clusters to achieve better visualisation.'''\n",
    "\n",
    "mc = MarkerCluster()# create empty cluster object\n",
    "\n",
    "for row in dataframe.itertuples():\n",
    " mc.add_child(folium.Marker(location=[row.latitude, row.longitude], #create markers and add to cluster\n",
    "     popup= folium.Popup(\n",
    "         folium.IFrame(\n",
    "             ('''Size: {Size} <br>\n",
    "              m2: {m2} <br>\n",
    "              Base Price: {bp} <br>\n",
    "              Utilities: {up} <br> \n",
    "              Total Price: {TotalPrice}'''\n",
    "              ).format(Size=row.Size, m2=row.m2, bp=row[5], up=row[6], TotalPrice=row[7]),\n",
    "              width=200, height=100)),\n",
    "     icon=folium.Icon(icon='home'))) #define icon symbol\n",
    " \n",
    "new_map.add_child(mc) \n",
    "new_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisations - Neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fileDir + '\\\\Data\\\\' + 'Praha.json') as data: #could be automated to get the data?\n",
    "    hoods = json.loads(data.read()) #available at @PrahaOpenData\n",
    "    \n",
    "gdf = gpd.GeoDataFrame.from_features(hoods[\"features\"]) #converting to geodataframe\n",
    "#gdf.crs = {'init': 'epsg:4326'}\n",
    "print(gdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_indv = gpd.GeoDataFrame(dataframe, geometry = gpd.points_from_xy(dataframe.longitude, dataframe.latitude))\n",
    "print(gdf_indv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = gpd.sjoin(gdf_indv, gdf, op='within') #geomerging polygons and points\n",
    "df_final = df_final.loc(axis=1)['Size', 'm2', 'Street', 'District', 'Base Price', 'Address', 'location', 'latitude',\n",
    "       'longitude', 'geometry', 'index_right', 'OBJECTID', 'PLOCHA', 'ID', 'NAZEV_MC',\n",
    "       'KOD_MO', 'TID_TMMESTSKECASTI_P', 'NAZEV_1', 'Shape_Length', 'Shape_Area']\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[\"m2\"] = df_final['m2'].apply(pd.to_numeric)\n",
    "avg_prices = df_final.loc(axis=1)['NAZEV_MC','Base Price','m2', 'District'].groupby(['NAZEV_MC']).mean()\n",
    "avg_prices['Median Price'] = df_final.loc(axis=1)['NAZEV_MC','Base Price'].groupby(['NAZEV_MC']).median()\n",
    "avg_prices['Price per Square Metre'] = avg_prices['Base Price'] / avg_prices['m2']\n",
    "\n",
    "\n",
    "avg_prices.columns = ['Price', 'm2', 'Median_price', 'Avg_m2']\n",
    "#avg_prices[\"AvgPrice/m2\"] = avg_prices['AvgPrice/m2'].apply(pd.to_numeric)\n",
    "avg_prices = avg_prices.round()\n",
    "avg_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the GeoDataframe object with the neighborhood summary data (neighborhood)\n",
    "merged = pd.merge(gdf, avg_prices, on='NAZEV_MC', how='left')\n",
    "print(merged.head())\n",
    "#merged = merged.replace('NaN', '0', regex=True)\n",
    "# Fill the null values\n",
    "values = {'Price': 0, 'm2': 0}\n",
    "#          'sf_mean': 0, 'price_sf_mean': 0, 'min_income': 0}\n",
    "merged = merged.fillna(value=values)\n",
    "\n",
    "# Convert to json\n",
    "merged_json = json.loads(merged.to_json())\n",
    "\n",
    "# Convert to json preferred string-like object \n",
    "json_data = json.dumps(merged_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dictionary contains the formatting for the data in the plots\n",
    "format_data = [('Price', 10000, 25000,'0,0', 'Price'),\n",
    "              ('Median_price', 10000, 25000,'0,0', 'Median Price'),\n",
    "              ('Avg_m2', 180, 350,'0,0', 'Price per Square Metre')] #more options to be added later\n",
    " \n",
    "#Create a DataFrame object from the dictionary \n",
    "format_df = pd.DataFrame(format_data, columns = ['field' , 'min_range', 'max_range' , 'format', 'verbage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plotting function\n",
    "def make_plot(field_name):    \n",
    "  # Set the format of the colorbar\n",
    "  min_range = format_df.loc[format_df['field'] == field_name, 'min_range'].iloc[0]\n",
    "  max_range = format_df.loc[format_df['field'] == field_name, 'max_range'].iloc[0]\n",
    "  field_format = format_df.loc[format_df['field'] == field_name, 'format'].iloc[0]\n",
    "\n",
    "  # Instantiate LinearColorMapper that linearly maps numbers in a range, into a sequence of colors.\n",
    "  color_mapper = LinearColorMapper(palette = palette, low = min_range, high = max_range)\n",
    "\n",
    "  # Create color bar.\n",
    "  format_tick = NumeralTickFormatter(format=field_format)\n",
    "  color_bar = ColorBar(color_mapper=color_mapper, label_standoff=18, formatter=format_tick,\n",
    "  border_line_color=None, location = (0, 0))\n",
    "\n",
    "  # Create figure object.\n",
    "  verbage = format_df.loc[format_df['field'] == field_name, 'verbage'].iloc[0]\n",
    "\n",
    "  p = figure(title = 'Apartment Rental ' + verbage + ' by City Parts in Prague', \n",
    "             plot_height = 650, plot_width = 850,\n",
    "             toolbar_location = None)\n",
    "  p.xgrid.grid_line_color = None\n",
    "  p.ygrid.grid_line_color = None\n",
    "  p.axis.visible = False\n",
    "\n",
    "  # Add patch renderer to figure. \n",
    "  p.patches('xs','ys', source = geosource, fill_color = {'field' : field_name, 'transform' : color_mapper},\n",
    "          line_color = 'black', line_width = 0.25, fill_alpha = 1)\n",
    "  \n",
    "  # Specify color bar layout.\n",
    "  p.add_layout(color_bar, 'right')\n",
    "\n",
    "  # Add the hover tool to the graph\n",
    "  p.add_tools(hover)\n",
    "  return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the callback function: update_plot\n",
    "def update_plot(attr, old, new):\n",
    "    # The input yr is the year selected from the slider\n",
    "    #yr = slider.value\n",
    "    new_data = json_data\n",
    "    \n",
    "    # The input cr is the criteria selected from the select box\n",
    "    cr = select.value\n",
    "    input_field = format_df.loc[format_df['verbage'] == cr, 'field'].iloc[0]\n",
    "    \n",
    "    # Update the plot based on the changed inputs\n",
    "    p = make_plot(input_field)\n",
    "    \n",
    "    # Update the layout, clear the old document and display the new document\n",
    "    layout = column(p, widgetbox(select))\n",
    "    #layout = column(p, widgetbox(select), widgetbox(slider))\n",
    "    curdoc().clear()\n",
    "    curdoc().add_root(layout)\n",
    "    \n",
    "    # Update the data\n",
    "    geosource.geojson = new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input geojson source that contains features for plotting for:\n",
    "# initial year 2018 and initial criteria sale_price_median\n",
    "geosource = GeoJSONDataSource(geojson = json_data)\n",
    "input_field = 'Price'\n",
    "\n",
    "# Define a sequential multi-hue color palette. Red color for Prague city color.\n",
    "palette = brewer['Reds'][8]\n",
    "\n",
    "# Reverse color order so that dark blue is highest obesity.\n",
    "palette = palette[::-1]\n",
    "\n",
    "# Add hover tool\n",
    "hover = HoverTool(tooltips = [ ('Neighborhood','@NAZEV_MC'),\n",
    "                              ('Average Price', '@Price'),\n",
    "                              ('Median Price', '@Median_price'),\n",
    "                              ('Average m2', '@m2'),\n",
    "                              ('Price per Square Metre', '@Avg_m2')])\n",
    "\n",
    "# Call the plotting function\n",
    "p = make_plot(input_field)\n",
    "# Make a slider object: slider \n",
    "#slider = Slider(title = 'Year',start = 2009, end = 2018, step = 1, value = 2018)\n",
    "#slider.on_change('value', update_plot)\n",
    "\n",
    "# Make a selection object: select\n",
    "select = Select(title='Select Criteria:', value='Price', options=['Price', 'Median Price',\n",
    "                                                                               'Price per Square Metre'])\n",
    "select.on_change('value', update_plot)\n",
    "\n",
    "# Make a column layout of widgetbox(slider) and plot, and add it to the current document\n",
    "# Display the current document\n",
    "layout = column(p, widgetbox(select))\n",
    "#layout = column(p, widgetbox(select), widgetbox(slider))\n",
    "curdoc().add_root(layout)\n",
    "\n",
    "output_notebook()\n",
    "show(p)\n",
    "\n",
    "#bokeh serve --show Downloader.ipynb -after streamlining the code for full functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What needs to be done\n",
    "1. Rerun geocoding for the dataframe which is without duplicates (so rerun the whole skript bsaically)\n",
    "2. Fix relative path for files - my usage of MAC ducked up the file storing locations. This will be more complicated - we need to slice the code into multiple .py files and then import the functions/files from outside either as files or as libraries. Generally, we should have one .py file for Downloader, one for Geocoding, and one for Visualizer. All data (downloaded or created) should be stored within the project repository in a folder called Data. This was functioning on my PC but my MAC killed the relative path storage :/\n",
    "3. Fix str(item) to r''\n",
    "4. Streamline the code - what can be written as a function, should be a function\n",
    "5. Maybe improve Class syntax - not necessary\n",
    "6. Figure out a way how to store and load data consecutively - so we can introduce a slider into the graph where a person could see average prices across times of his choosing (not a priority)\n",
    "    - for this, maybe look into SQL databases lecture\n",
    "    - main idea - download data everyday, save them then based on the date of download. Right now the code is only a snapshot of any given time\n",
    "9. Other data included in the interactive graph? Currently only price, median price, price/m2\n",
    "10. Streamline/write better code anywhere where legit\n",
    "11. Put the Visualizer.py on a web. It's quite easy to put it on a local server, simply by running bokeh serve --show Visualizer.ipynb. This is also a reason for slicing the code. \n",
    "12. Why does one get data fct , printing descrips and one printing apartments?\n",
    "\n",
    "\n",
    "Data output:\n",
    " - source_links.txt - file with htmls from real estate webs\n",
    " - source.json - parsed apartment data from htmls\n",
    " - geo_df.pkl - geocoded addresses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geo_env]",
   "language": "python",
   "name": "conda-env-geo_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
