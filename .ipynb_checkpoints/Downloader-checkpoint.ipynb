{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from gensim.utils import deaccent\n",
    "import random\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geocoding\n",
    "from bokeh.io import output_notebook, show, output_file\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import GeoJSONDataSource, LinearColorMapper, ColorBar, NumeralTickFormatter\n",
    "from bokeh.palettes import brewer\n",
    "\n",
    "from bokeh.io.doc import curdoc\n",
    "from bokeh.models import Slider, HoverTool, Select\n",
    "from bokeh.layouts import widgetbox, row, column\n",
    "import geopy\n",
    "import geopandas as gpd\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualisation\n",
    "import folium\n",
    "import folium.plugins as plugins\n",
    "from folium.plugins import MarkerCluster\n",
    "#from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_soups(links, name):\n",
    "        '''\n",
    "        This function iterates over all search pages, converts them into a BeautifulSoup object and stores them in a JSON file as \n",
    "        outside of this script. The keys of the dictoniary distinguish here between the different objects/HTML-pages. \n",
    "        '''\n",
    "        count = 0\n",
    "        dict_ = {}\n",
    "        soups = []\n",
    "        for link in tqm(links):\n",
    "            sleep(random.uniform(0.5, 2))\n",
    "            request = requests.get(link)\n",
    "            request.encoding='UTF-8'\n",
    "            soups.append(BeautifulSoup(request.text,'lxml'))\n",
    "        for soup in soups:\n",
    "            dict_[count] = str(deaccent(soup).encode(\"utf-8\"))\n",
    "            count += 1\n",
    "        with open(name, 'w') as write_file:\n",
    "            json.dump(dict_, write_file, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data from websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "'''Create a new Folder \"Data\" in the current working directory to store & access the data files which will be produced throughout this script'''\n",
    "newfolder = r'Data' \n",
    "if not os.path.exists(newfolder): #if already exists will not be created again\n",
    "    os.makedirs(newfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloaderBezRealitky(): \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        For the bezrealitky search, you need to iterate over search pages. Self.page_bezrealitky stores the maximum amount\n",
    "        of pages and then via self.link a list of all pages from search is created in self.hrefs_bezrealitky.\n",
    "        '''\n",
    "        self.link = 'https://www.bezrealitky.cz/vypis/nabidka-pronajem/byt/praha?_token=pr1lf-vKwDFfmFbICiz2PfC-Zdwq-2JolXi4MeMHsrw&page=1'\n",
    "        self.request = requests.get(self.link)\n",
    "        self.request.encoding='UTF-8'\n",
    "        self.soup = BeautifulSoup(self.request.text,'lxml')\n",
    "        self.page_bezrealitky = int(self.soup.findAll('a',{'class':'page-link pagination__page'})[-2].text)\n",
    "        self.hrefs_bezrealitky = ['https://www.bezrealitky.cz/vypis/nabidka-pronajem/byt/praha?_token=pr1lf-vKwDFfmFbICiz2PfC-Zdwq-2JolXi4MeMHsrw&page=' \n",
    "                                  + str(i) for i in range(1,self.page_bezrealitky)]\n",
    "        self.soups = []\n",
    "        self.counter = 0 #counter\n",
    "\n",
    "    def get_data(self):\n",
    "        '''\n",
    "        Main method to obtain and transform the data. HTMLs are read from the JSON file and stored in a list (soup_list) \n",
    "        within this script. Next, the method iterates over the list, converts the strings in the list into a BeautifulSoup\n",
    "        object and parses the html for relevant data. At the end, a nested dictionary (dicts) is created and stored\n",
    "        as a json file outside of this script.\n",
    "        '''\n",
    "        with open(fileDir + '\\\\Data\\\\bezrealitky_links.json', 'r', encoding='utf-8') as f:\n",
    "            content = json.load(f)\n",
    "        soup_list = list(content.values())\n",
    "        dicts = {}\n",
    "        counter = 0\n",
    "        for soup in tqm(soup_list):\n",
    "            descrips = [] #empty list for apartment values\n",
    "            values = [] #empty list for apartment prices\n",
    "            vals = BeautifulSoup(soup,'lxml').findAll('strong', {'class':'product__value'}) #parsing for apartment values\n",
    "            ##vals = soup.findAll('strong', {'class':'product__value'})\n",
    "            for vl in vals:\n",
    "                values.append(vl.text.strip())\n",
    "            #img = soup.findAll('img')\n",
    "            img = BeautifulSoup(soup,'lxml').findAll('img') #parsing for apartment info (street, city, size..)\n",
    "            for i in img:\n",
    "                if 'Pronajem' and 'obr. c. 1' in i['alt']: #info present at all pictures, let's take info from the first one\n",
    "                        info = i['alt'].split(',')[0:4] #info separated by comma, split into a list\n",
    "                        if 'Praha' == info[-1].strip(): #if street non present, insert a NaN instead\n",
    "                            info.insert(2, 'NaN')\n",
    "                            del info[-1]\n",
    "                            m = info[1].split(' ')\n",
    "                            info[1] = m[1]\n",
    "                            descrips.append(info)\n",
    "                        else:\n",
    "                            m = info[1].split(' ')\n",
    "                            info[1] = m[1]\n",
    "                            descrips.append(info)\n",
    "            count = 0\n",
    "            for pp in values: #append apartment prices to info about apartments in list descrips\n",
    "                try:\n",
    "                    descrips[count].append(pp)\n",
    "                    descrips[count][0] = descrips[count][0][-4:].strip()\n",
    "                    count += 1\n",
    "                except IndexError:\n",
    "                    count += 1\n",
    "                    continue\n",
    "            for item in descrips:\n",
    "                try:\n",
    "                    if '+' in item[4]: #prices often written as '19000 Kč + 4000Kč' so we need to split it\n",
    "                        prices = item.pop(4).split('+')\n",
    "                        item.append(re.sub(\"[^0-9]\", \"\", prices[0])) #keep only numeric characters, i.e. price\n",
    "                        item.append(re.sub(\"[^0-9]\", \"\", prices[1]))\n",
    "                    else:\n",
    "                        prices = [item.pop(4), '0'] #if only '19000 Kč', insert 0 as price for utilities not specified\n",
    "                        item.append(re.sub(\"[^0-9]\", \"\", prices[0]))\n",
    "                        item.append(re.sub(\"[^0-9]\", \"\", prices[1]))\n",
    "                except IndexError:\n",
    "                    continue\n",
    "            for item in descrips: #store apartment info, price into a dictionary and index by counter\n",
    "                try:\n",
    "                    dict = {}\n",
    "                    dict['Size'] = item[0]\n",
    "                    dict['m2'] = re.sub(\"[^0-9]\", \"\", item[1]) #keep only size, i.e. numeric characters\n",
    "                    dict['Street'] = deaccent(item[2]) #deaccent to provent potential errors\n",
    "                    dict['District'] = deaccent(item[3])\n",
    "                    dict['Base Price'] = int(item[4])\n",
    "                    dict['Utilities Price'] = int(item[5])\n",
    "                    dict['Total Price'] = int(item[4]) + int(item[5])\n",
    "                    dict['Source'] = 'bezrealitky.cz'\n",
    "                    dicts[self.counter] = dict\n",
    "                    self.counter += 1\n",
    "                except IndexError:\n",
    "                    #counter +=1\n",
    "                    continue\n",
    "            print('Done loop number ' + str(self.counter) + '. Printing descrips.')\n",
    "        with open(fileDir + '\\\\Data\\\\bezrealitky.json', 'w') as write_file: #store data into a json file\n",
    "            json.dump(dicts, write_file, indent = 4)\n",
    "\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DownloaderBezRealitky()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_soups(a.hrefs_bezrealitky[0:4], fileDir + '\\\\Data\\\\bezrealitky_links.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloaderReality():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        For the reality search, you need to iterate over search pages. Self.page_reality stores the maximum amount\n",
    "        of pages and then via self.link a list of all pages from search is created in self.hrefs_reality.\n",
    "        '''\n",
    "        self.link = 'https://reality.idnes.cz/s/pronajem/byty/praha/?page=1'\n",
    "        self.request = requests.get(self.link)\n",
    "        self.request.encoding='UTF-8'\n",
    "        self.soup = BeautifulSoup(self.request.text,'lxml')\n",
    "        self.page_reality = int(self.soup.findAll('a',{'class':'btn btn--border paging__item'})[-1].text) - 1\n",
    "        self.hrefs_reality = ['https://reality.idnes.cz/s/pronajem/byty/praha/?page=' \n",
    "                                  + str(i) for i in range(1,self.page_reality)]\n",
    "        self.soups = []\n",
    "        self.counter = 1000000 #counter - high number here to ensure that they get unique keys (ergo different from bezrealitky if that was executed first)\n",
    "        \n",
    "    def get_data(self):\n",
    "        '''\n",
    "        Main method to obtain and transform the data. HTMLs are read from the JSON file and stored in a list (soup_list) \n",
    "        within this script. Next, the method iterates over the list, converts the strings in txt file into a BeautifulSoup\n",
    "        object and parses the html for relevant data. At the end, a nested dictionary (dicts) is created and stored\n",
    "        as a json file outside of this script.\n",
    "        '''\n",
    "        '''\n",
    "        with open(fileDir + '\\\\Data\\\\reality_idnes_links.txt', 'r') as f:\n",
    "            content = f.read()\n",
    "        soup_list = content.split('BREAKHERE')\n",
    "        '''\n",
    "        with open(fileDir + '\\\\Data\\\\reality_idnes_links.json', 'r', encoding='utf-8') as f:\n",
    "            content = json.load(f)\n",
    "        soup_list = list(content.values())\n",
    "        dicts = {}\n",
    "        counter = 0\n",
    "        for soup in tqm(soup_list):\n",
    "            descrips = [] #empty list for apartment values\n",
    "            values = [] #empty list for apartment prices\n",
    "            info_size = []\n",
    "            apartments = []\n",
    "            vals = BeautifulSoup(soup,'lxml').findAll('p', {'class':'c-list-products__price'}) #parsing for apartment values\n",
    "            for vl in vals: #adding values\n",
    "                values.append(re.sub(\"[^0-9]\", \"\",vl.find('strong').text))\n",
    "                \n",
    "            locs = BeautifulSoup(soup,'lxml').findAll('p', {'class':'c-list-products__info'})\n",
    "            for i in locs: #adding location\n",
    "                if 'Komercni sdeleni' in i.text:\n",
    "                    continue\n",
    "                else:\n",
    "                    temp_info = str(i.text)\n",
    "                    temp_info = re.sub(r'^(?:\\\\n)+','', temp_info).strip()[:-2]\n",
    "                    temp_info = temp_info.strip().split(',')\n",
    "                    temp_info = [i.strip() for i in temp_info]\n",
    "                    if len(temp_info) == 1:\n",
    "                        temp_info.append(temp_info[0])\n",
    "                        temp_info[0] = 'NaN'\n",
    "                    if len(temp_info) == 3:\n",
    "                        del temp_info[2]\n",
    "                    descrips.append(temp_info)\n",
    "                    \n",
    "            sizes = BeautifulSoup(soup,'lxml').findAll('h2', {'class':'c-list-products__title'})\n",
    "            for s in sizes: #adding size and m2\n",
    "                try:\n",
    "                    item = s.text.split('bytu')[1].strip()[:-2]\n",
    "                    temp = item.split(',')\n",
    "                    temp[1] = temp[1][:-10].strip()\n",
    "                    info_size.append(temp)\n",
    "                except IndexError:\n",
    "                    continue\n",
    "            \n",
    "            for apart in range(0,len(info_size)):\n",
    "                apartments.append(info_size[apart] + descrips[apart] + [values[apart]])\n",
    "                \n",
    "            for item in apartments: #store apartment info, price into a dictionary and index by counter\n",
    "                try:\n",
    "                    dict = {}\n",
    "                    dict['Size'] = item[0]\n",
    "                    dict['m2'] = item[1]\n",
    "                    dict['Street'] = deaccent(item[2]) #deaccent to provent potential errors\n",
    "                    dict['District'] = deaccent(item[3])\n",
    "                    dict['Base Price'] = int(item[4])\n",
    "                    dict['Utilities Price'] = 0\n",
    "                    dict['Total Price'] = int(item[4])\n",
    "                    dict['Source'] = 'reality.idnes.cz'\n",
    "                    dicts[self.counter] = dict\n",
    "                    self.counter +=1\n",
    "                except ValueError:\n",
    "                    #counter += 1\n",
    "                    continue\n",
    "            print('Done loop number ' + str(self.counter) + '. Printing apartments.')\n",
    "        with open(fileDir + '\\\\Data\\\\idnes_reality.json', 'w') as write_file: #store data into a json file\n",
    "            json.dump(dicts, write_file, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = DownloaderReality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_soups(b.hrefs_reality[0:4], fileDir + '\\\\Data\\\\reality_idnes_links.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining the fetched data into one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_combine(*args):\n",
    "    big_dict = []\n",
    "    data = {}\n",
    "    #input example - 'idnes_reality.json', 'bezrealitky.json'\n",
    "    for arg in args:\n",
    "        with open(fileDir + '\\\\Data\\\\' + arg) as json_file:\n",
    "            file_ = json.load(json_file)\n",
    "            big_dict.append(file_)\n",
    "    for dt in big_dict:\n",
    "        data.update(dt)\n",
    "    return data\n",
    "\n",
    "data_combine('bezrealitky.json', 'idnes_reality.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(data_file):\n",
    "    '''\n",
    "    The clean_dataframe function takes a data file (here a dictoniary) as an input and returns a pandas dataframe, which is cleaned up and ready to use. \n",
    "    In particular, NaN values are replaced with nothing, white spaces before and after strings in the columns which have strings are removed \n",
    "    (which is important for the duplicate search), Rows which are duplicates (ergo same flat) are removed, the removal is executed based on the columns\n",
    "    Size, m2, Street and Total Price as it is highly likely that in case each of these values is identical the flat is identical and a new column 'Address'\n",
    "    is created which is necessary for geocoding.\n",
    "    '''\n",
    "    df = pd.DataFrame(data_file).T\n",
    "    df = df.replace('NaN', '', regex=True)\n",
    "    for name in ['Size','Street','District']: #strips all white spaces before and after strings\n",
    "        df[name]=df[name].str.strip()\n",
    "    print('Number of (removed) duplicates: ' + str(df.duplicated(['Size', 'm2', 'Street', 'Total Price']).sum()))\n",
    "    df = df.drop_duplicates(['Size', 'm2', 'Street', 'Total Price'], ignore_index=True) #drops duplicates \n",
    "    df['Address'] = df['Street'] + ',' + df['District'] + ',' + 'Praha' #creating address column for geocoding\n",
    "    return df\n",
    "\n",
    "dataframe = clean_dataframe(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locator = geopy.Nominatim(user_agent='myGeocoder')\n",
    "#locator = geopy.GoogleV3(api_key='AIzaSyDgWSTfwvVV3AELge6lJCw8hT0T4TwejYc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the GPS addresses\n",
    "geocode = RateLimiter(locator.geocode, min_delay_seconds=1) #this process takes about 2,5 hours\n",
    "\n",
    "'''We use Nominatim, an open source geocoding provider to retrive the locations (latitude, longitude, altitude) for our apartments. \n",
    "For this we provide Nominatim with the Addresses of the aparments.'''\n",
    "\n",
    "dataframe['location'] = dataframe['Address'].apply(geocode)\n",
    "\n",
    "dataframe['point'] = dataframe['location'].apply(lambda loc: tuple(loc.point) if loc else None)\n",
    "\n",
    "dataframe[['latitude', 'longitude', 'altitude']] = pd.DataFrame(dataframe['point'].tolist(), index=dataframe.index)\n",
    "\n",
    "dataframe = dataframe.dropna()\n",
    "dataframe.to_pickle(fileDir + '\\\\Data\\\\' +'geo_df.pkl', protocol = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation - Apartment locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FoliumMap(df_):\n",
    "    '''\n",
    "    Function creates a new, empty map with folium, the map doesnt contain any datapoints yet but is intialized at the mean latitude & longitude\n",
    "    point in our dataset. Then adds data points = flats (markers) to the map. For each observation (row) of the dataset we read the latitude & longitude\n",
    "    to create an icon which will be a display for the flat on the map. Furthermore we add a pop up text with basic information\n",
    "    to each icon. We create clusters to achieve better visualisation.\n",
    "    '''\n",
    "    new_map = folium.Map(location=[df_['latitude'].mean(), df_['longitude'].mean()], \n",
    "                        zoom_start=12,\n",
    "                        tiles='cartodbpositron')\n",
    "    mc = MarkerCluster()# create empty cluster object\n",
    "    for row in df_.itertuples():\n",
    "     mc.add_child(folium.Marker(location=[row.latitude, row.longitude], #create markers and add to cluster\n",
    "         popup= folium.Popup(\n",
    "             folium.IFrame(\n",
    "                 ('''Size: {Size} <br>\n",
    "                  m2: {m2} <br>\n",
    "                  Base Price: {bp} <br>\n",
    "                  Utilities: {up} <br> \n",
    "                  Total Price: {TotalPrice}'''\n",
    "                  ).format(Size=row.Size, m2=row.m2, bp=row[5], up=row[6], TotalPrice=row[7]),\n",
    "                  width=200, height=100)),\n",
    "         icon=folium.Icon(icon='home'))) #define icon symbol\n",
    "    new_map.add_child(mc) \n",
    "    new_map.save(outfile='folium_map.html') #saves file \n",
    "    return new_map\n",
    "\n",
    "dataframe = pd.read_pickle(fileDir + '\\\\Data\\\\' +'geo_df.pkl') #load from here\n",
    "\n",
    "FoliumMap(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisations - Neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborhoodsVisuals(): \n",
    "    def __init__(self, geo_filename='Praha.json', df = dataframe):\n",
    "        '''\n",
    "        NeighborhoodsVisuals takes a dataframe containing information about apartments and their geographic location and data (as a JSON file) \n",
    "        about the geographic location of Neighborhoods (here of Prague) as input and its methods combine this data, compute average values \n",
    "        for the neighborhoods and store them as a JSON file.  \n",
    "        '''\n",
    "        with open(fileDir + '\\\\Data\\\\' + geo_filename, encoding=\"utf8\") as data: \n",
    "                        hoods = json.loads(data.read()) \n",
    "        self.gdf = gpd.GeoDataFrame.from_features(hoods[\"features\"])\n",
    "        self.gdf_indv = gpd.GeoDataFrame(df, geometry = gpd.points_from_xy(df.longitude, df.latitude))\n",
    "        self.avg_prices()\n",
    "        self.store_merged()\n",
    "                \n",
    "    def avg_prices(self):\n",
    "        '''\n",
    "        Joins individual apartment geographic-dataframe with geographic-dataframe of neighborhoods and then calculates summary statistics for neighborhoods. If called prints dataframe.\n",
    "        '''\n",
    "        df_ = gpd.sjoin(self.gdf_indv, self.gdf, op='within') \n",
    "        df_ = df_.loc(axis=1)['Size', 'm2', 'Street', 'District', 'Base Price', 'Address', 'location', 'latitude',\n",
    "            'longitude', 'geometry', 'index_right', 'OBJECTID', 'PLOCHA', 'ID', 'NAZEV_MC',\n",
    "            'KOD_MO', 'TID_TMMESTSKECASTI_P', 'NAZEV_1', 'Shape_Length', 'Shape_Area']\n",
    "        df_[\"m2\"] = df_['m2'].apply(pd.to_numeric)\n",
    "        avg_price = df_.loc(axis=1)['NAZEV_MC','Base Price','m2'].groupby(['NAZEV_MC']).mean().round()\n",
    "        avg_price['Median Price'] = df_.loc(axis=1)['NAZEV_MC','Base Price'].groupby(['NAZEV_MC']).median().round()\n",
    "        avg_price['Crown per m2'] = (avg_price['Base Price'] / avg_price['m2']).round()\n",
    "        avg_price['Number of Apartments'] = df_.loc(axis=1)['NAZEV_MC','Base Price'].groupby(['NAZEV_MC']).count()\n",
    "        avg_price.columns = ['Price', 'm2', 'Median_price', 'Avg_m2_price', 'Number_of_Apartments']\n",
    "        return avg_price\n",
    "    \n",
    "    def store_merged(self):\n",
    "        # Merge the GeoDataframe object with the neighborhood summary data (neighborhood)\n",
    "        merged = pd.merge(self.gdf, self.avg_prices(), on='NAZEV_MC', how='left')\n",
    "        merged = merged.fillna(value={'Price': 0, 'm2': 0})\n",
    "        # Convert to json preferred string-like object \n",
    "        json_data = json.dumps(json.loads(merged.to_json()))\n",
    "        with open(fileDir + '\\\\Data\\\\merged_visuals_data.json', 'w') as write_file: #store data into a json file\n",
    "            json.dump(json_data, write_file, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = NeighborhoodsVisuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.avg_prices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dictionary contains the formatting for the data in the plots, which will be used in the interactive neighborhood map\n",
    "format_data = [('Price', 10000, 25000,'0,0', 'Price'),\n",
    "              ('Median_price', 10000, 25000,'0,0', 'Median Price'),\n",
    "              ('Avg_m2_price', 180, 350,'0,0', 'Price per Square Metre')] #more options to be added later\n",
    " \n",
    "#Create a DataFrame object from the dictionary \n",
    "format_df = pd.DataFrame(format_data, columns = ['field' , 'min_range', 'max_range' , 'format', 'verbage'])\n",
    "\n",
    "# Add hover tool, which will be used in the interactive neighborhood map\n",
    "hover = HoverTool(tooltips = [ ('Neighborhood','@NAZEV_MC'),\n",
    "                               ('Average Price', '@Price'),\n",
    "                               ('Median Price', '@Median_price'),\n",
    "                               ('Average m2', '@m2'),\n",
    "                               ('Crown per m2', '@Avg_m2_price'),\n",
    "                               ('Number of Apartments', '@Number_of_Apartments')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plotting function\n",
    "def make_plot(field_name, color='Reds'):\n",
    "        with open(fileDir + '\\\\Data\\\\merged_visuals_data.json', 'r', encoding='utf-8') as f:\n",
    "                json_data = json.load(f)\n",
    "\n",
    "        geosource = GeoJSONDataSource(geojson = json_data)\n",
    "          # Set the format of the colorbar\n",
    "        min_range = format_df.loc[format_df['field'] == field_name, 'min_range'].iloc[0]\n",
    "        max_range = format_df.loc[format_df['field'] == field_name, 'max_range'].iloc[0]\n",
    "        field_format = format_df.loc[format_df['field'] == field_name, 'format'].iloc[0]\n",
    "\n",
    "        #Define a sequential multi-hue color palette. Red color for Prague city color.\n",
    "        palette = brewer[color][8]\n",
    "\n",
    "        # Reverse color order so that dark red is highest obesity.\n",
    "        palette = palette[::-1]\n",
    "\n",
    "        # Instantiate LinearColorMapper that linearly maps numbers in a range, into a sequence of colors.\n",
    "        color_mapper = LinearColorMapper(palette = palette, low = min_range, high = max_range)\n",
    "\n",
    "        # Create color bar.\n",
    "        format_tick = NumeralTickFormatter(format=field_format)\n",
    "        color_bar = ColorBar(color_mapper=color_mapper, label_standoff=18, formatter=format_tick,\n",
    "        border_line_color=None, location = (0, 0))\n",
    "\n",
    "        # Create figure object.\n",
    "        verbage = format_df.loc[format_df['field'] == field_name, 'verbage'].iloc[0]\n",
    "\n",
    "        p = figure(title = 'Apartment Rental ' + verbage + ' by City Parts in Prague', \n",
    "                    plot_height = 650, plot_width = 850,\n",
    "                    toolbar_location = None)\n",
    "        p.xgrid.grid_line_color = None\n",
    "        p.ygrid.grid_line_color = None\n",
    "        p.axis.visible = False\n",
    "\n",
    "        # Add patch renderer to figure. \n",
    "        p.patches('xs','ys', source = geosource, fill_color = {'field' : field_name, 'transform' : color_mapper},\n",
    "                  line_color = 'black', line_width = 0.25, fill_alpha = 1)\n",
    "\n",
    "        # Specify color bar layout.\n",
    "        p.add_layout(color_bar, 'right')\n",
    "\n",
    "        # Add the hover tool to the graph\n",
    "        p.add_tools(hover)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the callback function: update_plot\n",
    "def update_plot(attr, old, new):\n",
    "    # The input yr is the year selected from the slider\n",
    "    #yr = slider.value\n",
    "    new_data = json_data\n",
    "    \n",
    "    # The input cr is the criteria selected from the select box\n",
    "    cr = select.value\n",
    "    input_field = format_df.loc[format_df['verbage'] == cr, 'field'].iloc[0]\n",
    "    \n",
    "    # Update the plot based on the changed inputs\n",
    "    p = make_plot(input_field)\n",
    "    \n",
    "    # Update the layout, clear the old document and display the new document\n",
    "    layout = column(p, widgetbox(select))\n",
    "    #layout = column(p, widgetbox(select), widgetbox(slider))\n",
    "    curdoc().clear()\n",
    "    curdoc().add_root(layout)\n",
    "    \n",
    "    # Update the data\n",
    "    geosource.geojson = new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fileDir + '\\\\Data\\\\merged_visuals_data.json', 'r', encoding='utf-8') as f:\n",
    "    json_data = json.load(f)\n",
    "# \n",
    "geosource = GeoJSONDataSource(geojson = json_data)\n",
    "input_field = 'Median_price'\n",
    "\n",
    "# Call the plotting function\n",
    "p = make_plot(input_field)\n",
    "\n",
    "# Make a selection object: select\n",
    "select = Select(title='Select Criteria:', value='Price', options=['Price', 'Median Price',\n",
    "                                                                               'Price per Square Metre'])\n",
    "select.on_change('value', update_plot)\n",
    "\n",
    "# Make a column layout of widgetbox(slider) and plot, and add it to the current document\n",
    "# Display the current document\n",
    "layout = column(p, widgetbox(select))\n",
    "\n",
    "curdoc().add_root(layout)\n",
    "\n",
    "output_notebook()\n",
    "show(p)\n",
    "\n",
    "#bokeh serve --show Downloader.ipynb -after streamlining the code for full functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What needs to be done\n",
    "1. Rerun geocoding for the dataframe which is without duplicates (so rerun the whole skript bsaically)\n",
    "!2. Fix relative path for files - my usage of MAC ducked up the file storing locations. This will be more complicated - we need to slice the code into multiple .py files and then import the functions/files from outside either as files or as libraries. Generally, we should have one .py file for Downloader, one for Geocoding, and one for Visualizer. All data (downloaded or created) should be stored within the project repository in a folder called Data. This was functioning on my PC but my MAC killed the relative path storage :/\n",
    "3. Fix str(item) to r''\n",
    "!4. Streamline the code - what can be written as a function, should be a function\n",
    "!5. Maybe improve Class syntax - not necessary\n",
    "!6. Figure out a way how to store and load data consecutively - so we can introduce a slider into the graph where a person could see average prices across times of his choosing (not a priority)\n",
    "    - for this, maybe look into SQL databases lecture\n",
    "    - main idea - download data everyday, save them then based on the date of download. Right now the code is only a snapshot of any given time\n",
    "!9. Other data included in the interactive graph? Currently only price, median price, price/m2\n",
    "!10. Streamline/write better code anywhere where legit\n",
    "11. Put the Visualizer.py on a web. It's quite easy to put it on a local server, simply by running bokeh serve --show Visualizer.ipynb. This is also a reason for slicing the code. \n",
    "!12. Why does one get data fct , printing descrips and one printing apartments?\n",
    "13. delete the comments in the visualisation part\n",
    "!14. How exactly got  Praha.json data - @PrahaOpenData could download from there?\n",
    "15. Add progress bar for longer running functions\n",
    "\n",
    "\n",
    "\n",
    "Data output:\n",
    " - source_links.txt - file with htmls from real estate webs\n",
    " - source.json - parsed apartment data from htmls\n",
    " - geo_df.pkl - geocoded addresses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
